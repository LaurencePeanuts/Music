% ====================================================================%
%  ctu2015.tex
%
%  Adapted from notes written in January 2015
%  by Michael D. Schneider, Roland de Putter
%  with input from Ryan Keisler and Phil Marshall.
%
% ====================================================================

\documentclass[11pt, letterpaper]{article}

\usepackage[OT1]{fontenc}
\usepackage[in]{fullpage}
\usepackage{natbib}

\usepackage[,bookmarks,bookmarksopen,colorlinks,linkcolor={black},citecolor={black},urlcolor={red}]{hyperref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsmath,amssymb}
\usepackage{aas_macros}
\usepackage{mathptmx}  % times roman, including math (where possible)
\usepackage{dcolumn}
\usepackage{latexsym,mathrsfs}
\usepackage{color}
\usepackage{hyperref}
\usepackage{tabu}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=blue,
}

\include{macros}

% ====================================================================

\begin{document}

\title{Statistical Model for the CtU2015 Hack: `The Music of the Sphere'}

\author{Michael Schneider, Roland de Putter, Ryan Keisler, Phil Marshall}

\date{\today}

\maketitle
% \tableofcontents


Our goal is to draw posterior samples of the large-scale 3D cosmological mass
density (or gravitational potential) in our universe, given a map of the CMB.

% --------------------------------------------------------------------

\section{Preliminary Notes}

The observed CMB temperature $\data$ is modeled as a sum of a signal $\signal$
and $\noise$, neglecting foregrounds,
\begin{equation}
	\data = \signal + \noise,
\end{equation}
where both $\signal$ and $\noise$ are Gaussian distributed with zero means
and covariances $\Smat$ and $\Nmat$.

We want to sample realizations of the 3D gravitational potential $\gravpot$
given the observed 2D CMB map $\data$. We can model the relationship between
the potential and the CMB temperature as a linear transfer function matrix
$\transfer$,
\begin{equation}
	\signal \equiv \transfer \gravpot.
\end{equation}

The likelihood is a Gaussian distribution given our assumption of a
Gaussian noise model,
\begin{equation}
	\pr(\data | \signal, \Nmat) = \normdist_{\data}
	\left(\signal, \Nmat\right)
	= \normdist_{\data} \left(\transfer \gravpot, \Nmat\right)
\end{equation}

The prior on the gravitational potential is defined as
\begin{equation}
	\pr(\gravpot | \Psimat) =
	\normdist_{\gravpot} \left(0, \Psimat\right)
\end{equation}

% --------------------------------------------------------------------

\subsection{Conditional posteriors for Gibbs sampling}

\citet{Wandelt:2003uk} shows that the conditional distribution
of the signal or gravitational potential is a Gaussian with a
mean and covariances (also known as the `Wiener filter'),
\begin{equation}\label{eq:psi_cond_post}
	\pr(\gravpot | \data, \Psimat, \Nmat)
	= \normdist_{\gravpot}
	\left(\muv, \Sigmamat\right),
\end{equation}
where,
\begin{align}
	\muv &\equiv \Psimat
	\left(\Psimat + \Cmat_{N}\right)^{-1}\mv
	\label{eq:psi_cond_mean}
	\\
	\Sigmamat &\equiv
	\Psimat
	\left(\Psimat + \Cmat_{N}\right)^{-1} \Cmat_{N},
	\label{eq:psi_cond_cov}
\end{align}
\begin{align}
	\mv &\equiv
	\left(\transfer^{T} \Nmat^{-1}\transfer\right)^{-1}
	\transfer^{T}\Nmat^{-1} \data
\end{align}
is the maximum likelihood estimator of the signal and,
\begin{equation}
	\Cmat_{N} \equiv \left(\transfer^{T} \Nmat^{-1}\transfer\right)^{-1}.
\end{equation}

% --------------------------------------------------------------------

\subsection{Summary for code implementation}

Our goal is to sample conditional Gaussian draws from \autoref{eq:psi_cond_post}.
An important point is that the vector $\muv$ can be extended from just those
voxels that encompass the last scattering surface to also those voxels
filling the rest of the universe. Then, posterior inference of the mass density
away from the last scattering surface is described by a conditional
multivariate Gaussian distribution with covariance matrix given by the
Schur complement of $\Sigmamat$.

The chief computational challenge is inverting the matrices in
\autoref{eq:psi_cond_mean} and \autoref{eq:psi_cond_cov}.
\citet{Wandelt:2003uk} and \citet{Wandelt:2004ul} describe
a conjugate gradient based method to evaluate the Wiener filter terms for
Gibbs sampling.

A secondary computational issue is specification of the discretized
matrix $\transfer$ which represents the transfer function relating the
3D graviational potential to the 2D CMB anisotropies.

% We an probably start with non-optimized matrix inverses in a suitably
% scaled toy model and expand to larger numbers of pixels as time
% and interest permit.

% --------------------------------------------------------------------

\section{Detailed Notes}

We will jump, not always in a well-organized way, between the general
notation from \cite{zaroubietal95} and a more practical notation for
the problem at hand (the simplest version of it).

Let's start with the following toy model. We have a 3D potential field on a grid
\be
\Pot({\bf x}_i).
\ee
Here $i$ labels the grid points so that we can treat this field as a ``theory vector'',
\be
{\bf s} \equiv \Potvector,
\ee
with components
\be
s_i = \Pot_i = \Pot({\bf x}_i) \quad i = 1,..,N.
\ee
Moreover, we have a $M$-dimensional data vector, given by some linear operator on the theory vector, plus noise,
\be\label{eq:data_model}
{\bf d} = \transfer \, {\bf s} + {\bf \epsilon}.
\ee
Here $\transfer$ is the transformation matrix defining the data, and the noise
has covariance
\be
N^{\epsilon}_{kl} \equiv \langle \epsilon_k \, \epsilon_l \rangle.
\ee
I will try to use the indices $i,j$ on the theory (i.e.~$\Pot$) side (so that these indices run from $1$ to $N$)
and use $k,l$ on the data side (so that they run from $1$ to $M$).
More concretely, let's say the data are simply measurements of $\Pot({\bf x}_i)$ at
a subset of specific
grid locations $\{i_1,...,i_M \}$,
\be
d_k \equiv \hat{\Pot}({\bf x}_{i_k}) = \Pot({\bf x}_{i_k}) + \epsilon_k = s_{i_k} + \epsilon_k.
\ee
Then, ${\bf R}$ is a $M \times N$ matrix with entries
\be
R_{ki} \equiv \delta^{K}_{i_k i}.
\ee
For simplicity, we can start with a diagonal noise matrix,
\be
N^{\epsilon}_{kl} = \sigma^2_\epsilon \, \delta^{K}_{k l}
\ee

Now, given a measurement of $\data$, i.e.~given a noisy measurement
of the potential at a subset of grid locations, for instance on a sphere,
we want to generate realizations of the full field. There is one more ingredient we need:
the prior covariance matrix of the theory vector.
This is a $N \times N$ matrix $\smat$, with entries
\be\label{eq:signal_cov}
S_{ij} \equiv \langle s_i s_j \rangle = \langle \Pot({\bf x}_i) \, \Pot({\bf x}_j) \rangle = \xi_\Pot(|{\bf x}_i - {\bf x}_j|).
\ee

Now, following e.g.~\cite{rp92,zaroubietal95}, we can first construct the maximum likelihood estimator
for the field $\Pot({\bf x}_i)$.
\be
\label{eq:mv estimator}
{\bf s}^{\rm MV} = {\bf F} \, {\bf d},
\ee
with
\be
\label{eq:F matrix}
{\bf F} = \langle {\bf s} \, {\bf d}^\dagger \rangle \, \langle {\bf d} \, {\bf d}^\dagger \rangle^{-1} = {\bf S} \, {\bf R}^\dagger \left( {\bf R} \, {\bf S} \, {\bf R}^\dagger + {\bf N^\epsilon}  \right)^{-1}
\ee
Since we assume Gaussianity, this estimator gives the posterior expectation value for the field
and the maximum likelihood solution.

The above Eqs.~(\ref{eq:mv estimator})-(\ref{eq:F matrix}) give the general result. Things become clearer when we apply it to our specific example.
For instance,
\be
\langle d_k \, d_l \rangle = \left({\bf R} \, {\bf S} \, {\bf R}^\dagger  + {\bf N^\epsilon} \right)_{kl} = \xi_\Pot(|{\bf x}_{i_k} - {\bf x}_{i_l}|) + \sigma_{\epsilon}^2 \, \delta^K_{kl} \equiv \left({\bf \xi_\Pot} + \sigma^2_\epsilon \, {\bf I}\right)_{kl}
\ee
(${\bf I}$ is the identity matrix and ${\bf \xi_\Pot}$ is the correlation function matrix restricted to the grid points
where we have measurements, e.g. the CMB last-scattering surface) and
\be
\langle s_i \, d_k \rangle = \left( {\bf S} \, {\bf R}^\dagger \right)_{i k} = \xi_\Pot(|{\bf x}_{i} - {\bf x}_{i_k}|).
\ee
We can't proceed further analytically, because we need to invert the matrix ${\bf \xi_\Pot} + \sigma^2_\epsilon \, {\bf I}$,
which is simply the total covariance matrix of the observable $d_k \equiv \hat{\Pot}({\bf x}_{i_k})$.
We can now write for the solution,
\be
\label{eq:mv sol}
s^{\rm MV}_i = \Pot^{\rm MV}({\bf x}_i) = \sum_{kl}  \xi_\Pot(|{\bf x}_{i} - {\bf x}_{i_k}|) \, \left({\bf \xi_\Pot} + \sigma^2_\epsilon \, {\bf I}\right)^{-1}_{kl} \, \hat{\Pot}({\bf x}_l)
\ee


%S_{ij} = xi_ij
%R=MxM unity + zeros, (MxN)
%Rdag = MxM unity + zeros (NxM)
%Neps = unity (MxM)
%RSRdag = xi_kl,

%in parentheses:

%xi_{kl}

%S Rdag = xi_ik

%sherman-morrison
%eps = A - M b^T
%epsinv = ...

%in fourier space,
%S diagonal, either R not or N complicated,

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsection{Generating realizations of $\Pot({\bf x}_i)$}
\label{sub:generating_realizations}

Now that we have the {\it mean} of the conditional posterior of ${\bf s} = {\bf \Pot}$, given in Eq.~(\ref{eq:mv sol}), there is a straightforward
method for generating realizations that follow the posterior distribution:
\begin{itemize}
\item
Generate an unconstrained Gaussian random realization of the theory field, ${\bf \tilde{s}} = {\bf \tilde{\Pot}}$ (i.e.~the 3D potential), that follows the prior
correlation function of the field, i.e.~the same way of generating random realizations we use for the true field, that is our
input to the whole calculation.
\item
Generate a (Gaussian) realization of the measurement noise ${\bf \epsilon}$, that follows the covariance
given by ${\bf N^{\epsilon}}$.
\item
Construct a realization of the data:
\be
{\bf \tilde{d}} = {\bf R} \, {\bf \tilde{s}} + {\bf \epsilon}.
\ee
Specifically, for us this means
\be
\tilde{\hat{\Pot}}({\bf x}_{i_k}) = \tilde{\Pot}({\bf x}_{i_k}) + \tilde{\epsilon}_k.
\ee
\item
Write the realization of the field, {\it conditional on the observations}, as
\be
{\bf s}_{\rm post} = {\bf s}^{\rm MV} + {\bf \tilde{s}} - {\bf F} \, {\bf \tilde{d}}.
\ee
For us this means
\be
\Pot_{\rm post}({\bf x}_i) = \Pot^{\rm MV}({\bf x}_i) + \tilde{\Pot}({\bf x}_i) - \tilde{\Pot}^{\rm MV}({\bf x}_i),
\ee
where I have written
\be
\tilde{\Pot}^{\rm MV}({\bf x}_i) \equiv \left({\bf F} \, {\bf \tilde{d}}\right)_i = \sum_{kl}  \xi_\Pot(|{\bf x}_{i} - {\bf x}_{i_k}|) \, \left({\bf \xi_\Pot} + \sigma^2_\epsilon \, {\bf I}\right)^{-1}_{kl} \, \tilde{\hat{\Pot}}({\bf x}_l)
\ee
This quantity has the appropriate statistics given the measurement ${\bf d}$ and
the prior statistics of ${\bf s}$ and of the noise.
\end{itemize}

Possibly more advanced method that would be interesting to explore: \cite{jaslav15}.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsection{Informing simulations with existing data} % (fold)
\label{sub:informing_simulations_with_existing_data}

Consider the common scenario that we want to constrain cosmological parameters $\theta$
given a data set $\data$, which is related to the cosmological mass density as in
\autoref{eq:data_model}.
If we have no knowledge about the realized mass density $\Pot$ in our observed portion of the
universe then we must marginalize over all possible mass density realizations in computing the
marginal posterior of $\theta$.

However, if we have performed previous surveys of some volumes of the universe, we should be
able to use the information from those surveys to constrain the mass density marginalization in
the analysis of our current survey.
Specifically, the joint marginal likelihood of our current data $\data$ and an `external' or
past data set $\dataext$ is,
\begin{equation}\label{eq:joint_likelihood}
	P(\data, \dataext | \theta) =
	P(\data | \dataext, \theta) P(\dataext | \theta).
\end{equation}
That is, we want a \emph{conditional} inference of the data $\data$ given the external data $\dataext$,
which factors the joint data likelihood.

The posterior on the cosmological parameters $\theta$ factors as,
\begin{align}
	P(\theta | \data, \dataext) &= \frac{P(\data, \dataext | \theta) P(\theta)}{P(\data,\dataext)}
	\notag\\
	&= \frac{P(\data | \dataext,\theta) P(\dataext | \theta) P(\theta)}{P(\data | \dataext) P(\dataext)}
	\notag\\
	&= \frac{P(\data | \dataext,\theta) P(\theta | \dataext)}{P(\data | \dataext)}.
	\label{eq:factored_posterior}
\end{align}
So, we can infer posterior constraints on $\theta$ given both past $\dataext$ and current $\data$ data sets
by evaluating only the likelihood of the current data conditioned on the external data $P(\data|\dataext,\theta)$
as long as we update our prior on $\theta$ from the posterior inferences given only the external data sets.

If we assume the observational noise is uncorrelated between the current $\data$ and past $\dataext$ data sets,
the likelihood conditioned on the mass density factors,
\begin{equation}
	P(\data,\dataext | \Pot, \theta) = P(\data | \Pot, \theta) P(\dataext | \Pot, \theta).
\end{equation}
But, after marginalizing over the mass density the joint likelihood no longer factors,
\begin{equation}\label{eq:marg_likelihood}
	P(\data, \dataext | \theta) =
	\int d\Pot\, P(\data | \Pot, \theta) P(\dataext | \Pot, \theta) P(\Pot | \theta).
\end{equation}

We can evaluate \autoref{eq:marg_likelihood} and thereby understand the impact of adding information
from a past data set $\dataext$ by assuming Gaussian distributions for each conditional distribution
in the integrand,
\begin{align}
	P(\Pot | \theta) &\equiv \normdist_{\Pot}(0, \smat(\theta))
	\label{eq:ic_prior}\\
	P(\data | \Pot, \theta) &\equiv
		\normdist_{\data} \left(\transfer(\theta)\Pot, \noisemat\right)
	\label{eq:cond_likelihood}\\
	P(\dataext | \Pot, \theta) &\equiv
		\normdist_{\dataext} \left(\transferext(\theta)\Pot, \noisematext\right).
	\label{eq:cond_ext_likelihood}
\end{align}
The Gaussian distribution in \autoref{eq:ic_prior} comes from our assumption that the
initial conditions for the cosmological mass density perturbations are Gaussian with mean zero and
a covariance $\smat(\theta)$ given by the two-point correlation function as in \autoref{eq:signal_cov}.
The Gaussian distributions in \autoref{eq:cond_likelihood} and \autoref{eq:cond_ext_likelihood} come from
the assumed noise models for each data set.

By first rewriting the likelihood as a function of $\Pot$,
\begin{equation}
	\normdist_{\data} \left(\transfer\Pot, \noisemat\right) =
	\left|\transfer\transfer^{T}\right|^{1/2}
	\normdist_{\Pot} \left(\transfer^{-1}\data, \left(\transfer^{T}\noisematinv\transfer\right)^{-1}\right)
\end{equation}
and then multiplying by the prior on $\Pot$,
\begin{equation}\label{eq:cond_posterior}
	P(\data|\Pot,\theta)P(\Pot|\theta) =
	\left|\transfer\transfer^{T}\right|^{1/2}
	\normdist_{\transfer\data} \left(0, \smat + \noisemattransfer\right)
	\normdist_{\Pot} \left(\cmat \noisemattransferinv \transfer \data, \cmat\right)
\end{equation}
we get another Gaussian distribution in $\Pot$ with maximum given by \autoref{eq:F matrix}.
And, in \autoref{eq:cond_posterior} we defined,
\begin{align}
	\noisemattransfer &\equiv \left(\transfer^{T} \noisematinv \transfer\right)^{-1},
	\\
	\cmat &\equiv \left(\smat^{-1} + \noisemattransferinv\right)^{-1}.
	\label{eq:cmat}
\end{align}

Performing similar manipulations with the Gaussian likelihood of $\dataext$, we can
perform the integral in \autoref{eq:marg_likelihood} as the integral of Gaussian distribution to get,
\begin{multline}\label{eq:marg_likelihood_final}
	P(\data, \dataext | \theta) =
	% External data likelihood
	\left[
	\left|\transferext\transferext^{T}\right|^{1/2}
	\normdist_{\transferext\dataext} \left(0, \smat + \noisemattransferext\right)
	\right]
	\\ \times
	% Conditional likelihood
	\left[
	\left|\transfer\transfer^{T}\right|^{1/2}
	\normdist_{\transfer\data} \left(\cmatext\noisemattransferextinv\transferext\dataext,
	\noisemattransfer + \cmatext\right)
	\right],
\end{multline}
where the term in the first square brackets is recognized as $P(\dataext | \theta)$ while the
second set of square brackets gives $P(\data | \dataext, \theta)$.

The result for $P(\dataext|\theta)$ in \autoref{eq:marg_likelihood_final} is familiar. The
marginal likelihood for a mass density tracer without any extra prior information has mean zero
and a covariance that is the sum of the signal and noise covariance matrices. Although, the
prefactor depending on the transfer function $\transferext$ is not usually kept even though it
does, in general, depend on the cosmological parameters $\theta$.

The result for $P(\data|\dataext,\theta)$ in \autoref{eq:marg_likelihood_final} is a Gaussian
distribution in the data with a mean determined by the external data $\dataext$, suitably filtered
by the signal and noise covariances. The covariance of $\data$ is a sum
of the noise covariance for the data $\noisemat$ and the matrix $\cmatext$ as defined in
\autoref{eq:cmat} (inserting the past data noise covariance). Because the \emph{inverse}
covariances are summed in \autoref{eq:cmat},
\begin{equation}
	\cmatext \approx \noisemattransferext
\end{equation}
when the past data $\dataext$ is measured with high signal-to-noise ratio (i.e., as the observational
noise on $\dataext$ becomes arbitrarily small). So, even though measurements of $\dataext$ may be
sample variance dominated, the conditional inferences from $\data$ given $\dataext$ need not be
limited by sample variance. This is our main result.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsubsection{Algorithm for simulations}

This suggests a procedure for running simulations of cosmological large-scale structure for
parameter inference,
\begin{enumerate}
	\item Infer $P(\theta | \dataext)$ given past or external data sets $\dataext$ as is typically done.
	\item Use $P(\Pot, \theta | \dataext)$ as in \autoref{sub:generating_realizations} to generate
	initial conditions for cosmological simulations to aid in the inferences from a new
	data set $\data$ that at least partially overlaps in volume with $\dataext$.
	\item Infer cosmological parameter constraints given $\data$ given the likelihood function
	obtained from the previous step and replacing the usual prior on $\theta$ with $P(\theta | \dataext)$.
\end{enumerate}
From \autoref{eq:factored_posterior}, this procedure is guaranteed to yield correct inferences
from $P(\theta | \data, \dataext)$ (i.e., as if we had analyzed both data sets jointly).
But, we obtain the computational savings in running \emph{conditional} simulations given
the first data set $\dataext$.

% --------------------------------------------------------------------

\bibliographystyle{apj}
\bibliography{references}

% --------------------------------------------------------------------

\end{document}

% ====================================================================
